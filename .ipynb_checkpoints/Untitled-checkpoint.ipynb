{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d214b15d-e8f3-42bc-b690-1aa89db6d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1006)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import pytesseract  # For OCR using Tesseract\n",
    "import time         # For measuring execution time\n",
    "import pdf2image    # For converting PDF pages to images\n",
    "from PIL import Image  # For image processing\n",
    "import requests     # For API calls to Anthropic\n",
    "import difflib      # For text comparison\n",
    "import numpy as np  # For numerical operations\n",
    "import cv2          # For advanced image processing\n",
    "import matplotlib.pyplot as plt  # For visualizations\n",
    "import io           # For handling byte streams\n",
    "import base64       # For encoding/decoding binary data\n",
    "from dotenv import load_dotenv  # For loading environment variables from .env file\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import os           # For interacting with the operating system\n",
    "import logging      # For logging messages\n",
    "import concurrent.futures  # For parallel execution of tasks\n",
    "from requests.adapters import HTTPAdapter  # For configuring HTTP requests\n",
    "from requests.packages.urllib3.util.retry import Retry  # For implementing retry logic in HTTP requests\n",
    "from google.cloud import vision # For the Google Cloud Vision client library\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import Levenshtein  # For efficient edit distance calculations\n",
    "import anthropic\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from fuzzywuzzy import fuzz\n",
    "from abc import ABC, abstractmethod\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd2e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_WORKERS = os.cpu_count() or 4 # Adjust workers to the number of CPU cores available of current system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3d13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    page_number: int\n",
    "    x: float\n",
    "    y: float\n",
    "    width: float\n",
    "    height: float\n",
    "    text: str\n",
    "    confidence: Optional[float] = None\n",
    "    type: Optional[str] = None  # For Mathpix: 'line' or 'word'\n",
    "\n",
    "@dataclass\n",
    "class OCRPageResult:\n",
    "    page_number: int\n",
    "    text: str\n",
    "    bounding_boxes: List[BoundingBox]\n",
    "    processing_time: float\n",
    "    avg_confidence: Optional[float] = None  # For Tesseract\n",
    "\n",
    "@dataclass\n",
    "class OCRResult:\n",
    "    ocr_text: str\n",
    "    page_results: List[OCRPageResult]\n",
    "    total_time: float\n",
    "    ocr_engine: str\n",
    "\n",
    "@dataclass\n",
    "class AnthropicPageResult:\n",
    "    page_number: int\n",
    "    text: str\n",
    "    processing_time: float\n",
    "\n",
    "@dataclass\n",
    "class AnthropicResult:\n",
    "    full_text: str\n",
    "    page_results: List[AnthropicPageResult]\n",
    "    total_time: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d635e36-72fb-4aab-8448-f49334334e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic API Key: sk-an...bBgAA\n"
     ]
    }
   ],
   "source": [
    "# LOAD ENVIRONMENT VARIABLES\n",
    "load_dotenv()\n",
    "\n",
    "# ANTHROPIC\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    raise ValueError(\"Please set the ANTHROPIC_API_KEY environment variable.\")\n",
    "\n",
    "print(f\"Anthropic API Key: {ANTHROPIC_API_KEY[:5]}...{ANTHROPIC_API_KEY[-5:]}\")  # Only print first and last 5 characters\n",
    "\n",
    "# Initialize the Anthropic client at the top of your script\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# MATHPIX\n",
    "MATHPIX_APP_ID = os.getenv('MATHPIX_APP_ID')\n",
    "MATHPIX_APP_KEY = os.getenv('MATHPIX_APP_KEY')\n",
    "\n",
    "if not MATHPIX_APP_ID or not MATHPIX_APP_KEY:\n",
    "    raise ValueError(\"Please set the MATHPIX_APP_ID and MATHPIX_APP_KEY environment variables.\")\n",
    "\n",
    "# GOOGLE CLOUD VISION\n",
    "GOOGLE_APPLICATION_CREDENTIALS = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "if not GOOGLE_APPLICATION_CREDENTIALS:\n",
    "    raise ValueError(\"Please set the GOOGLE_APPLICATION_CREDENTIALS environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b077effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For checkpointing\n",
    "def save_checkpoint(data, filename):\n",
    "       with open(filename, 'wb') as f:\n",
    "           pickle.dump(data, f)\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a70f93-383a-4f55-aeaa-64dead1bbc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Setup a session with retry strategy\n",
    "session = requests.Session()\n",
    "retry = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"POST\"]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74002a8-0cef-422d-b4e8-810d134bbcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(image, max_dim=1568):\n",
    "    \"\"\"\n",
    "    Converts a PIL Image to a base64-encoded string after resizing if necessary.\n",
    "\n",
    "    Parameters:\n",
    "    - image (PIL.Image): The image to convert.\n",
    "    - max_dim (int): Maximum dimension (width or height) in pixels.\n",
    "\n",
    "    Returns:\n",
    "    - str: Base64-encoded string of the image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Resize image if any dimension exceeds max_dim\n",
    "        if max(image.size) > max_dim:\n",
    "            ratio = max_dim / max(image.size)\n",
    "            new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
    "            image = image.resize(new_size, Image.LANCZOS)\n",
    "        \n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")  # Use PNG for lossless quality\n",
    "        img_bytes = buffered.getvalue()\n",
    "        img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "        return img_base64\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in image_to_base64: {e}\", exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9dc0db-9686-4538-b6ec-ce471a7b726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Preprocess the image to enhance OCR accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    - image (PIL.Image): The image to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image: The preprocessed image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to grayscale\n",
    "        image = image.convert('L')\n",
    "        \n",
    "        # Convert to NumPy array for OpenCV processing\n",
    "        image_np = np.array(image)\n",
    "        \n",
    "        # Apply denoising\n",
    "        image_np = cv2.fastNlMeansDenoising(image_np, h=30)\n",
    "\n",
    "        # Apply adaptive thresholding\n",
    "        image_np = cv2.adaptiveThreshold(image_np, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                        cv2.THRESH_BINARY, 11, 2)\n",
    "        \n",
    "        # Detect and correct skew\n",
    "        coords = np.column_stack(np.where(image_np > 0))\n",
    "        angle = cv2.minAreaRect(coords)[-1]\n",
    "        if angle < -45:\n",
    "            angle = -(90 + angle)\n",
    "        else:\n",
    "            angle = -angle\n",
    "        \n",
    "        (h, w) = image_np.shape\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        rotated = cv2.warpAffine(image_np, M, (w, h), flags=cv2.INTER_CUBIC, \n",
    "                                borderMode=cv2.BORDER_REPLICATE)\n",
    "        \n",
    "        # Convert back to PIL Image\n",
    "        preprocessed_image = Image.fromarray(image_np)\n",
    "        return preprocessed_image\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in preprocess_image: {e}\", exc_info=True)\n",
    "        return image  # Return original image if preprocessing fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b04abf-52b0-4ab5-a101-00562d21f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_images(pdf_path, dpi=300):\n",
    "    \"\"\"\n",
    "    Converts a PDF file to a list of PIL Image objects.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_path (str): Path to the PDF file.\n",
    "    - dpi (int): Resolution for the conversion.\n",
    "\n",
    "    Returns:\n",
    "    - list of PIL.Image: List of images representing each PDF page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        images = pdf2image.convert_from_path(pdf_path, dpi=dpi)\n",
    "        logger.info(f\"Converted PDF to {len(images)} images.\")\n",
    "        return images\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"PDF file not found: {pdf_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to convert PDF to images: {e}\", exc_info=True)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5353dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base OCR Class\n",
    "\n",
    "class BaseOCR(ABC):\n",
    "    def __init__(self, max_workers=MAX_WORKERS):\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_page(self, page_number, image):\n",
    "        pass\n",
    "\n",
    "    def perform_ocr(self, pdf_path):\n",
    "        images = convert_pdf_to_images(pdf_path)\n",
    "        num_pages = len(images)\n",
    "        logging.info(f\"Total pages to process: {num_pages}\")\n",
    "\n",
    "        if num_pages == 0:\n",
    "            logging.warning(\"No images extracted from PDF.\")\n",
    "            return OCRResult(ocr_text='', page_results=[], total_time=0.0, ocr_engine=self.__class__.__name__)\n",
    "\n",
    "        total_start_time = time.time()\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = {executor.submit(self.process_page, i, img): i for i, img in enumerate(images)}\n",
    "            \n",
    "            page_results = []\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=num_pages, desc=f\"Performing {self.__class__.__name__} OCR\"):\n",
    "                page_number = futures[future]\n",
    "                try:\n",
    "                    page_result = future.result()\n",
    "                    page_results.append(page_result)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing page {page_number + 1}: {e}\", exc_info=True)\n",
    "                    page_results.append(OCRPageResult(page_number=page_number+1, text='', bounding_boxes=[], processing_time=0.0))\n",
    "\n",
    "        total_time = time.time() - total_start_time\n",
    "        logging.info(f\"Total {self.__class__.__name__} OCR time: {total_time:.2f} seconds.\")\n",
    "\n",
    "        ocr_text = '\\n'.join(result.text for result in page_results)\n",
    "        return OCRResult(ocr_text=ocr_text, page_results=page_results, total_time=total_time, ocr_engine=self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f4f08a-c3d5-4734-93d6-c33332574212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(page_number, image):\n",
    "    \"\"\"\n",
    "    Processes a single PDF page: preprocesses image, sends API request, and retrieves text and processing time.\n",
    "\n",
    "    Parameters:\n",
    "    - page_number (int): The page number.\n",
    "    - image (PIL.Image): The image of the page.\n",
    "\n",
    "    Returns:\n",
    "    - AnthropicPageResult: Extracted text and processing time for the page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess image\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "        \n",
    "        # Convert image to base64\n",
    "        image_base64 = image_to_base64(preprocessed_image)\n",
    "        \n",
    "        # Prepare the message content\n",
    "        message_content = [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": \"image/png\",\n",
    "                    \"data\": image_base64,\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Output all the text in this page.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Make the API call using the Anthropic client\n",
    "        start_time = time.time()\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20240620\",\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": message_content,\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Extract the text from the response\n",
    "        text = message.content[0].text if message.content else ''\n",
    "\n",
    "        logging.info(f\"Page {page_number} processed in {elapsed_time:.2f} seconds.\")\n",
    "        return AnthropicPageResult(page_number=page_number, text=text, processing_time=elapsed_time)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing page {page_number}: {e}\", exc_info=True)\n",
    "        return AnthropicPageResult(page_number=page_number, text='', processing_time=0.0)\n",
    "\n",
    "    \n",
    "def get_anthropic_ground_truth(pdf_path, max_workers=5):\n",
    "    \"\"\"\n",
    "    Extracts ground truth text from a PDF using the Anthropic Vision API.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_path (str): Path to the PDF file.\n",
    "    - max_workers (int): Number of worker processes for parallel processing.\n",
    "\n",
    "    Returns:\n",
    "    - AnthropicResult: Extracted text, per-page results, and total processing time.\n",
    "    \"\"\"\n",
    "    images = convert_pdf_to_images(pdf_path)\n",
    "    \n",
    "    if not images:\n",
    "        logging.warning(\"No images to process.\")\n",
    "        return AnthropicResult(full_text='', page_results=[], total_time=0.0)\n",
    "    \n",
    "    num_pages = len(images)\n",
    "    logging.info(f\"Total pages to process with Anthropic: {num_pages}\")\n",
    "    \n",
    "    # Start total processing timing\n",
    "    start_total = time.time()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_page, page_number, image)\n",
    "            for page_number, image in enumerate(images, start=1)\n",
    "        ]\n",
    "        page_results = []\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing pages\"):\n",
    "            page_result = future.result()\n",
    "            page_results.append(page_result)\n",
    "    \n",
    "    # Sort page results by page number\n",
    "    page_results.sort(key=lambda x: x.page_number)\n",
    "    \n",
    "    # Concatenate all pages' text\n",
    "    full_text = '\\n'.join(result.text for result in page_results)\n",
    "    \n",
    "    # Calculate total processing time\n",
    "    total_time = time.time() - start_total\n",
    "    logging.info(f\"Total time to obtain ground truth: {total_time:.2f} seconds.\")\n",
    "    \n",
    "    return AnthropicResult(full_text=full_text, page_results=page_results, total_time=total_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4211e2ee-2700-44c0-82e5-4125e9fb24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TesseractOCR(BaseOCR):\n",
    "    def process_page(self, page_number, image):\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        custom_config = r'--oem 3 --psm 6'\n",
    "        text = pytesseract.image_to_string(preprocessed_image, config=custom_config, lang='eng')\n",
    "        data = pytesseract.image_to_data(preprocessed_image, output_type=pytesseract.Output.DICT, config=custom_config, lang='eng')\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        bounding_boxes = [\n",
    "            BoundingBox(\n",
    "                page_number=page_number+1,\n",
    "                x=data['left'][i],\n",
    "                y=data['top'][i],\n",
    "                width=data['width'][i],\n",
    "                height=data['height'][i],\n",
    "                text=data['text'][i],\n",
    "                confidence=data['conf'][i] if data['conf'][i] != -1 else None\n",
    "            )\n",
    "            for i in range(len(data['text']))\n",
    "            if data['text'][i].strip()\n",
    "        ]\n",
    "        \n",
    "        avg_confidence = np.mean([bb.confidence for bb in bounding_boxes if bb.confidence is not None])\n",
    "        \n",
    "        return OCRPageResult(page_number=page_number+1, text=text, bounding_boxes=bounding_boxes, processing_time=processing_time, avg_confidence=avg_confidence)\n",
    "\n",
    "\n",
    "# Define a helper function for OCR processing\n",
    "def ocr_page(page_number, image):\n",
    "    \"\"\"\n",
    "    Performs OCR on a single image and measures processing time.\n",
    "\n",
    "    Parameters:\n",
    "    - page_number (int): The page number.\n",
    "    - image (PIL.Image): The image to perform OCR on.\n",
    "\n",
    "    Returns:\n",
    "    - (text, data, processing_time, avg_confidence): Tuple containing OCR text, data, time taken, and average confidence.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess image using existing function\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Perform OCR to extract text with configuration\n",
    "        custom_config = r'--oem 3 --psm 6'  # Example configuration\n",
    "        text = pytesseract.image_to_string(preprocessed_image, config=custom_config, lang='eng')\n",
    "\n",
    "        # Perform OCR to extract data with bounding boxes and confidence levels\n",
    "        data = pytesseract.image_to_data(preprocessed_image, output_type=pytesseract.Output.DICT, config=custom_config, lang='eng')\n",
    "\n",
    "        # Extract confidence levels\n",
    "        confidences = data['conf']\n",
    "\n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        logging.info(f\"Page {page_number + 1} processed in {processing_time:.2f} seconds.\")\n",
    "\n",
    "        # Calculate average confidence for this page\n",
    "        valid_confidences = [conf for conf in confidences if conf != -1]  # -1 indicates no confidence available\n",
    "        avg_confidence = sum(valid_confidences) / len(valid_confidences) if valid_confidences else None\n",
    "\n",
    "        if avg_confidence is not None:\n",
    "            logging.info(f\"Page {page_number + 1} average confidence: {avg_confidence:.2f}\")\n",
    "        else:\n",
    "            logging.info(f\"Page {page_number + 1} average confidence: Not available\")\n",
    "\n",
    "        return text, data, processing_time, avg_confidence\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing page {page_number + 1}: {e}\")\n",
    "        return '', {}, 0.0, None\n",
    "\n",
    "def perform_tesseract_ocr(pdf_path, max_workers=MAX_WORKERS):\n",
    "    \"\"\"\n",
    "    Performs OCR on a PDF document using Tesseract and evaluates processing speed.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_path (str): Path to the PDF file.\n",
    "    - max_workers (int or None): Number of worker processes for parallel OCR.\n",
    "                                 If None, it will default to os.cpu_count().\n",
    "\n",
    "    Returns:\n",
    "    - ocr_text (str): Concatenated OCR text from all pages.\n",
    "    - ocr_data_pages (list): List of OCR data dictionaries with bounding boxes.\n",
    "    - total_time (float): Total time taken for OCR in seconds.\n",
    "    - per_page_times (list): List of processing times per page in seconds.\n",
    "    \"\"\"\n",
    "    ocr_text_pages = []\n",
    "    ocr_data_pages = []\n",
    "    per_page_times = []\n",
    "    per_page_confidences = []\n",
    "\n",
    "    # Convert PDF to images using existing function\n",
    "    images = convert_pdf_to_images(pdf_path)\n",
    "    num_pages = len(images)\n",
    "    logging.info(f\"Total pages to process: {num_pages}\")\n",
    "\n",
    "    if num_pages == 0:\n",
    "        logging.warning(\"No images extracted from PDF.\")\n",
    "        return '', [], 0.0, []\n",
    "\n",
    "    # Start total OCR timing\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(ocr_page, i, img): i for i, img in enumerate(images)}\n",
    "\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=num_pages, desc=\"Performing Tesseract OCR\"):\n",
    "            page_number = futures[future]\n",
    "            try:\n",
    "                text, data, proc_time, avg_confidence = future.result()\n",
    "                ocr_text_pages.append(text)\n",
    "                ocr_data_pages.append(data)\n",
    "                per_page_times.append(proc_time)\n",
    "                per_page_confidences.append(avg_confidence)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving result for page {page_number + 1}: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "\n",
    "    # End total OCR timing\n",
    "    total_end_time = time.time()\n",
    "    total_time = total_end_time - total_start_time\n",
    "    logging.info(f\"Total OCR time: {total_time:.2f} seconds.\")\n",
    "\n",
    "    # Concatenate all pages' text\n",
    "    ocr_text = '\\n'.join(ocr_text_pages)\n",
    "\n",
    "    page_results = []\n",
    "    for i, (text, data, proc_time, avg_confidence) in enumerate(zip(ocr_text_pages, ocr_data_pages, per_page_times, per_page_confidences)):\n",
    "        bounding_boxes = [\n",
    "            BoundingBox(\n",
    "                page_number=i+1,\n",
    "                x=data['left'][j],\n",
    "                y=data['top'][j],\n",
    "                width=data['width'][j],\n",
    "                height=data['height'][j],\n",
    "                text=data['text'][j],\n",
    "                confidence=data['conf'][j] if data['conf'][j] != -1 else None\n",
    "            )\n",
    "            for j in range(len(data['text']))\n",
    "            if data['text'][j].strip()\n",
    "        ]\n",
    "        page_results.append(OCRPageResult(\n",
    "            page_number=i+1,\n",
    "            text=text,\n",
    "            bounding_boxes=bounding_boxes,\n",
    "            processing_time=proc_time,\n",
    "            avg_confidence=avg_confidence\n",
    "        ))\n",
    "\n",
    "    return OCRResult(\n",
    "        ocr_text=ocr_text,\n",
    "        page_results=page_results,\n",
    "        total_time=total_time,\n",
    "        ocr_engine=\"Tesseract\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ecbad6-c771-46f3-a269-23c77d1da4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathpixOCR(BaseOCR):\n",
    "    def __init__(self, app_id, app_key, max_workers=MAX_WORKERS):\n",
    "        super().__init__(max_workers)\n",
    "        self.app_id = app_id\n",
    "        self.app_key = app_key\n",
    "\n",
    "    def process_page(self, page_number, image):\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "        image_base64 = image_to_base64(preprocessed_image)\n",
    "        \n",
    "        payload = {\n",
    "            \"src\": f\"data:image/png;base64,{image_base64}\",\n",
    "            \"formats\": [\"text\", \"data\"],\n",
    "            \"data_options\": {\n",
    "                \"include_word_data\": True\n",
    "            },\n",
    "            \"rm_spaces\": True\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"app_id\": self.app_id,\n",
    "            \"app_key\": self.app_key,\n",
    "            \"Content-type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        api_endpoint = \"https://api.mathpix.com/v3/text\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = requests.post(api_endpoint, json=payload, headers=headers)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        response_json = response.json()\n",
    "        \n",
    "        text = response_json.get('text', '')\n",
    "        bounding_boxes = self._extract_bounding_boxes(response_json, page_number)\n",
    "        \n",
    "        return OCRPageResult(page_number=page_number+1, text=text, bounding_boxes=bounding_boxes, processing_time=processing_time)\n",
    "\n",
    "    def _extract_bounding_boxes(self, response_json, page_number):\n",
    "        bounding_boxes = []\n",
    "        \n",
    "        # Extract Word Bounding Boxes\n",
    "        word_data = response_json.get('word_data', [])\n",
    "        for word in word_data:\n",
    "            if word.get('type') == 'text' and word.get('included', False):\n",
    "                cnt = word.get('cnt', [])\n",
    "                if cnt:\n",
    "                    xs = [point[0] for point in cnt]\n",
    "                    ys = [point[1] for point in cnt]\n",
    "                    bbox = BoundingBox(\n",
    "                        page_number=page_number + 1,\n",
    "                        x=min(xs),\n",
    "                        y=min(ys),\n",
    "                        width=max(xs) - min(xs),\n",
    "                        height=max(ys) - min(ys),\n",
    "                        text=word.get('text', ''),\n",
    "                        confidence=word.get('confidence', None),\n",
    "                        type='word'\n",
    "                    )\n",
    "                    bounding_boxes.append(bbox)\n",
    "        \n",
    "        return bounding_boxes\n",
    "    \n",
    "# Define a helper function for OCR processing\n",
    "def ocr_page_mathpix(page_number, image):\n",
    "    \"\"\"\n",
    "    Performs OCR on a single image using MathPix and extracts bounding boxes with confidence levels.\n",
    "\n",
    "    Parameters:\n",
    "    - page_number (int): The page number.\n",
    "    - image (PIL.Image): The image to perform OCR on.\n",
    "\n",
    "    Returns:\n",
    "    - (text, bounding_boxes, processing_time): Tuple containing OCR text,\n",
    "      bounding boxes with confidence, and time taken.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess image using existing function\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "        \n",
    "        # Convert image to base64\n",
    "        image_base64 = image_to_base64(preprocessed_image)\n",
    "        \n",
    "        # Prepare the JSON payload\n",
    "        payload = {\n",
    "            \"src\": f\"data:image/png;base64,{image_base64}\",\n",
    "            \"formats\": [\"text\", \"data\"],\n",
    "            \"data_options\": {\n",
    "                \"include_line_data\": True,\n",
    "                \"include_word_data\": True\n",
    "            },\n",
    "            \"rm_spaces\": True\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"app_id\": MATHPIX_APP_ID,\n",
    "            \"app_key\": MATHPIX_APP_KEY,\n",
    "            \"Content-type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        api_endpoint = \"https://api.mathpix.com/v3/text\"\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Send OCR request to MathPix\n",
    "        response = requests.post(api_endpoint, json=payload, headers=headers)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        response_json = response.json()\n",
    "        text = response_json.get('text', '')\n",
    "        \n",
    "        # Extract bounding boxes from line_data and word_data\n",
    "        bounding_boxes = []\n",
    "        \n",
    "        # Extract Line Bounding Boxes\n",
    "        line_data = response_json.get('line_data', [])\n",
    "        for line in line_data:\n",
    "            if line.get('included', False):\n",
    "                cnt = line.get('cnt', [])\n",
    "                # Calculate bounding box from contour points\n",
    "                if cnt:\n",
    "                    xs = [point[0] for point in cnt]\n",
    "                    ys = [point[1] for point in cnt]\n",
    "                    bbox = {\n",
    "                        \"page_number\": page_number + 1,\n",
    "                        \"type\": \"line\",\n",
    "                        \"x\": min(xs),\n",
    "                        \"y\": min(ys),\n",
    "                        \"width\": max(xs) - min(xs),\n",
    "                        \"height\": max(ys) - min(ys),\n",
    "                        \"text\": line.get('text', ''),\n",
    "                        \"confidence\": line.get('confidence', None),\n",
    "                        \"confidence_rate\": line.get('confidence_rate', None)\n",
    "                    }\n",
    "                    bounding_boxes.append(bbox)\n",
    "        \n",
    "        # Extract Word Bounding Boxes\n",
    "        word_data = response_json.get('word_data', [])\n",
    "        for word in word_data:\n",
    "            if word.get('type') == 'text' and word.get('included', False):\n",
    "                cnt = word.get('cnt', [])\n",
    "                if cnt:\n",
    "                    xs = [point[0] for point in cnt]\n",
    "                    ys = [point[1] for point in cnt]\n",
    "                    bbox = {\n",
    "                        \"page_number\": page_number + 1,\n",
    "                        \"type\": \"word\",\n",
    "                        \"x\": min(xs),\n",
    "                        \"y\": min(ys),\n",
    "                        \"width\": max(xs) - min(xs),\n",
    "                        \"height\": max(ys) - min(ys),\n",
    "                        \"text\": word.get('text', ''),\n",
    "                        \"confidence\": word.get('confidence', None),\n",
    "                        \"confidence_rate\": word.get('confidence_rate', None)\n",
    "                    }\n",
    "                    bounding_boxes.append(bbox)\n",
    "        \n",
    "        logging.info(f\"MathPix - Page {page_number + 1} processed in {elapsed_time:.2f} seconds.\")\n",
    "        return text, bounding_boxes, elapsed_time\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"MathPix API request failed for page {page_number + 1}: {e}\", exc_info=True)\n",
    "        return '', [], 0.0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"MathPix - Exception on page {page_number + 1}: {e}\", exc_info=True)\n",
    "        return '', [], 0.0\n",
    "    \n",
    "def perform_mathpix_ocr(pdf_path, max_workers=MAX_WORKERS):\n",
    "    \"\"\"\n",
    "    Performs OCR on a PDF document using MathPix and evaluates processing speed.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_path (str): Path to the PDF file.\n",
    "    - max_workers (int): Number of worker processes for parallel OCR.\n",
    "\n",
    "    Returns:\n",
    "    - ocr_text (str): Concatenated OCR text from all pages.\n",
    "    - bounding_boxes_all_pages (list): List of bounding boxes dictionaries for all pages.\n",
    "    - total_time (float): Total time taken for OCR in seconds.\n",
    "    - per_page_times (list): List of processing times per page in seconds.\n",
    "    \"\"\"\n",
    "    ocr_text_pages = []\n",
    "    bounding_boxes_all_pages = []\n",
    "    per_page_times = []\n",
    "    images = convert_pdf_to_images(pdf_path)\n",
    "    num_pages = len(images)\n",
    "    logging.info(f\"Total pages to process with MathPix: {num_pages}\")\n",
    "\n",
    "    if num_pages == 0:\n",
    "        logging.warning(\"No images extracted from PDF.\")\n",
    "        return '', [], 0.0, []\n",
    "\n",
    "    # Start total OCR timing\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all pages to the executor\n",
    "        futures = {executor.submit(ocr_page_mathpix, i, img): i for i, img in enumerate(images)}\n",
    "        \n",
    "        # Iterate over completed futures with a progress bar\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=num_pages, desc=\"Performing MathPix OCR\"):\n",
    "            page_number = futures[future]\n",
    "            try:\n",
    "                text, bounding_boxes, proc_time = future.result()\n",
    "                ocr_text_pages.append(text)\n",
    "                bounding_boxes_all_pages.extend(bounding_boxes)\n",
    "                per_page_times.append(proc_time)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving MathPix result for page {page_number + 1}: {e}\", exc_info=True)\n",
    "                ocr_text_pages.append('')\n",
    "                per_page_times.append(0.0)\n",
    "    \n",
    "    # End total OCR timing\n",
    "    total_end_time = time.time()\n",
    "    total_time = total_end_time - total_start_time\n",
    "    logging.info(f\"Total MathPix OCR time: {total_time:.2f} seconds.\")\n",
    "\n",
    "    # Concatenate all pages' text\n",
    "    ocr_text = '\\n'.join(ocr_text_pages)\n",
    "\n",
    "    page_results = []\n",
    "    for i, (text, bounding_boxes, proc_time) in enumerate(zip(ocr_text_pages, bounding_boxes_all_pages, per_page_times)):\n",
    "        page_bounding_boxes = [\n",
    "            BoundingBox(\n",
    "                page_number=bbox['page_number'],\n",
    "                x=bbox['x'],\n",
    "                y=bbox['y'],\n",
    "                width=bbox['width'],\n",
    "                height=bbox['height'],\n",
    "                text=bbox['text'],\n",
    "                confidence=bbox['confidence'],\n",
    "                type=bbox['type']\n",
    "            )\n",
    "            for bbox in bounding_boxes if bbox['page_number'] == i+1\n",
    "        ]\n",
    "        page_results.append(OCRPageResult(\n",
    "            page_number=i+1,\n",
    "            text=text,\n",
    "            bounding_boxes=page_bounding_boxes,\n",
    "            processing_time=proc_time\n",
    "        ))\n",
    "\n",
    "    return OCRResult(\n",
    "        ocr_text=ocr_text,\n",
    "        page_results=page_results,\n",
    "        total_time=total_time,\n",
    "        ocr_engine=\"Mathpix\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8dbd684-262a-4d2a-8583-48f9ebaf6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleCloudVisionOCR(BaseOCR):\n",
    "    def __init__(self, max_workers=MAX_WORKERS):\n",
    "        super().__init__(max_workers)\n",
    "        self.client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    def process_page(self, page_number, image):\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "        \n",
    "        img_byte_arr = io.BytesIO()\n",
    "        preprocessed_image.save(img_byte_arr, format='PNG')\n",
    "        img_bytes = img_byte_arr.getvalue()\n",
    "        \n",
    "        vision_image = vision.Image(content=img_bytes)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = self.client.text_detection(image=vision_image)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        if response.error.message:\n",
    "            raise Exception(f\"Google Vision API Error: {response.error.message}\")\n",
    "        \n",
    "        texts = response.text_annotations\n",
    "        full_text = texts[0].description if texts else ''\n",
    "        \n",
    "        bounding_boxes = [\n",
    "            BoundingBox(\n",
    "                page_number=page_number+1,\n",
    "                x=min(vertex.x for vertex in text.bounding_poly.vertices),\n",
    "                y=min(vertex.y for vertex in text.bounding_poly.vertices),\n",
    "                width=max(vertex.x for vertex in text.bounding_poly.vertices) - min(vertex.x for vertex in text.bounding_poly.vertices),\n",
    "                height=max(vertex.y for vertex in text.bounding_poly.vertices) - min(vertex.y for vertex in text.bounding_poly.vertices),\n",
    "                text=text.description,\n",
    "                confidence=text.confidence if hasattr(text, 'confidence') else None\n",
    "            )\n",
    "            for text in texts[1:]  # Skip the first one as it's the full text\n",
    "        ]\n",
    "        \n",
    "        return OCRPageResult(page_number=page_number+1, text=full_text, bounding_boxes=bounding_boxes, processing_time=processing_time)\n",
    "\n",
    "def ocr_page_google(page_number, image):\n",
    "    \"\"\"\n",
    "    Performs OCR on a single image using Google Cloud Vision, extracts bounding boxes with confidence scores,\n",
    "    and measures processing time.\n",
    "\n",
    "    Parameters:\n",
    "    - page_number (int): The page number.\n",
    "    - image (PIL.Image): The image to perform OCR on.\n",
    "\n",
    "    Returns:\n",
    "    - tuple:\n",
    "        - text (str): The full extracted text.\n",
    "        - bounding_boxes (list): List of dictionaries with 'text', 'bounding_box', and 'confidence'.\n",
    "        - elapsed_time (float): Time taken to process the page in seconds.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Google Cloud Vision client inside the worker\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "        \n",
    "        # Preprocess image using existing function\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "        \n",
    "        # Convert image to bytes\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        preprocessed_image.save(img_byte_arr, format='PNG')\n",
    "        img_bytes = img_byte_arr.getvalue()\n",
    "        \n",
    "        # Create Image object\n",
    "        vision_image = vision.Image(content=img_bytes)\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Perform text detection\n",
    "        response = client.text_detection(image=vision_image)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if response.error.message:\n",
    "            logging.error(f\"Google Vision - Error on page {page_number + 1}: {response.error.message}\")\n",
    "            return '', [], elapsed_time\n",
    "        \n",
    "        texts = response.text_annotations\n",
    "        if texts:\n",
    "            # The first text_annotation is the full text\n",
    "            full_text = texts[0].description\n",
    "            \n",
    "            # Extract bounding boxes and confidence scores for each detected text element (excluding the full text)\n",
    "            bounding_boxes = []\n",
    "            for text in texts[1:]:\n",
    "                bbox = [(vertex.x, vertex.y) for vertex in text.bounding_poly.vertices]\n",
    "                confidence = text.confidence if hasattr(text, 'confidence') else None\n",
    "                bounding_boxes.append({\n",
    "                    'text': text.description,\n",
    "                    'bounding_box': bbox,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "        else:\n",
    "            full_text = ''\n",
    "            bounding_boxes = []\n",
    "        \n",
    "        logging.info(f\"Google Vision - Page {page_number + 1} processed in {elapsed_time:.2f} seconds.\")\n",
    "        return full_text, bounding_boxes, elapsed_time\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Google Vision - Exception on page {page_number + 1}: {e}\")\n",
    "        return '', [], 0.0\n",
    "\n",
    "# Google Cloud Vision OCR Implementation\n",
    "def perform_google_cloud_vision_ocr(pdf_path, max_workers=MAX_WORKERS):\n",
    "    \"\"\"\n",
    "    Performs OCR on a PDF document using Google Cloud Vision and evaluates processing speed.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_path (str): Path to the PDF file.\n",
    "    - max_workers (int or None): Number of worker processes for parallel OCR.\n",
    "                                  If None, defaults to the number of CPU cores.\n",
    "\n",
    "    Returns:\n",
    "    - tuple:\n",
    "        - ocr_text (str): Concatenated OCR text from all pages.\n",
    "        - bounding_boxes_pages (list): List of bounding boxes per page.\n",
    "        - total_time (float): Total time taken for OCR in seconds.\n",
    "        - per_page_times (list): List of processing times per page in seconds.\n",
    "    \"\"\"\n",
    "    ocr_text_pages = []\n",
    "    bounding_boxes_pages = []\n",
    "    per_page_times = []\n",
    "    images = convert_pdf_to_images(pdf_path)\n",
    "    num_pages = len(images)\n",
    "    logging.info(f\"Total pages to process with Google Cloud Vision: {num_pages}\")\n",
    "\n",
    "    if num_pages == 0:\n",
    "        logging.warning(\"No images extracted from PDF.\")\n",
    "        return '', [], 0.0, []\n",
    "\n",
    "    # Start total OCR timing\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all pages to the executor\n",
    "        futures = {executor.submit(ocr_page_google, i, img): i for i, img in enumerate(images)}\n",
    "        \n",
    "        # Iterate over completed futures with a progress bar\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=num_pages, desc=\"Performing Google Cloud Vision OCR\"):\n",
    "            page_number = futures[future]\n",
    "            try:\n",
    "                text, bounding_boxes, proc_time = future.result()\n",
    "                ocr_text_pages.append(text)\n",
    "                bounding_boxes_pages.append(bounding_boxes)\n",
    "                per_page_times.append(proc_time)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error retrieving Google Cloud Vision result for page {page_number + 1}: {e}\", exc_info=True)\n",
    "                ocr_text_pages.append('')\n",
    "                bounding_boxes_pages.append([])\n",
    "                per_page_times.append(0.0)\n",
    "\n",
    "    # End total OCR timing\n",
    "    total_end_time = time.time()\n",
    "    total_time = total_end_time - total_start_time\n",
    "    logging.info(f\"Total Google Cloud Vision OCR time: {total_time:.2f} seconds.\")\n",
    "\n",
    "    # Concatenate all pages' text\n",
    "    ocr_text = '\\n'.join(ocr_text_pages)\n",
    "\n",
    "    page_results = []\n",
    "    for i, (text, bounding_boxes, proc_time) in enumerate(zip(ocr_text_pages, bounding_boxes_pages, per_page_times)):\n",
    "        page_bounding_boxes = [\n",
    "            BoundingBox(\n",
    "                page_number=i+1,\n",
    "                x=min(vertex[0] for vertex in bbox['bounding_box']),\n",
    "                y=min(vertex[1] for vertex in bbox['bounding_box']),\n",
    "                width=max(vertex[0] for vertex in bbox['bounding_box']) - min(vertex[0] for vertex in bbox['bounding_box']),\n",
    "                height=max(vertex[1] for vertex in bbox['bounding_box']) - min(vertex[1] for vertex in bbox['bounding_box']),\n",
    "                text=bbox['text'],\n",
    "                confidence=bbox['confidence']\n",
    "            )\n",
    "            for bbox in bounding_boxes\n",
    "        ]\n",
    "        page_results.append(OCRPageResult(\n",
    "            page_number=i+1,\n",
    "            text=text,\n",
    "            bounding_boxes=page_bounding_boxes,\n",
    "            processing_time=proc_time\n",
    "        ))\n",
    "\n",
    "    return OCRResult(\n",
    "        ocr_text=ocr_text,\n",
    "        page_results=page_results,\n",
    "        total_time=total_time,\n",
    "        ocr_engine=\"Google Cloud Vision\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "682b032c-ffda-40f0-ba9e-ab511b0a83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8367127e-0f14-4599-b4c0-13d6c9e6b428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 13:18:45,618 - INFO - OCR Evaluation starting...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m evaluation_df \u001b[38;5;241m=\u001b[39m load_checkpoint(evaluation_checkpoint)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluation_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     evaluation_df \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_all_ocrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     save_checkpoint(evaluation_df, evaluation_checkpoint)\n\u001b[1;32m    142\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOCR evaluation completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 85\u001b[0m, in \u001b[0;36mevaluate_all_ocrs\u001b[0;34m(ground_truth, ocr_results)\u001b[0m\n\u001b[1;32m     83\u001b[0m evaluations \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ocr_name, ocr_result \u001b[38;5;129;01min\u001b[39;00m ocr_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 85\u001b[0m     evaluations[ocr_name] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_ocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(evaluations)\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "Cell \u001b[0;32mIn[21], line 65\u001b[0m, in \u001b[0;36mevaluate_ocr\u001b[0;34m(ground_truth, ocr_result)\u001b[0m\n\u001b[1;32m     55\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCER\u001b[39m\u001b[38;5;124m'\u001b[39m: character_error_rate(gt_text, ocr_text),\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWER\u001b[39m\u001b[38;5;124m'\u001b[39m: word_error_rate(gt_text, ocr_text),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg_Time_Per_Page\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([page\u001b[38;5;241m.\u001b[39mprocessing_time \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m ocr_result\u001b[38;5;241m.\u001b[39mpage_results]),\n\u001b[1;32m     62\u001b[0m }\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Add ROUGE scores\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m rouge_results \u001b[38;5;241m=\u001b[39m \u001b[43mrouge_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m evaluation\u001b[38;5;241m.\u001b[39mupdate(rouge_results)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Calculate CWA if confidence scores are available\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m, in \u001b[0;36mrouge_scores\u001b[0;34m(reference, hypothesis)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate ROUGE scores.\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m rouge \u001b[38;5;241m=\u001b[39m Rouge()\n\u001b[0;32m---> 28\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mrouge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m'\u001b[39m: scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m'\u001b[39m: scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m'\u001b[39m: scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m }\n",
      "File \u001b[0;32m~/code/cadastral-take-home/venv/lib/python3.11/site-packages/rouge/rouge.py:107\u001b[0m, in \u001b[0;36mRouge.get_scores\u001b[0;34m(self, hyps, refs, avg, ignore_empty)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(hyps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(refs))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m avg:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_avg_scores(hyps, refs)\n",
      "File \u001b[0;32m~/code/cadastral-take-home/venv/lib/python3.11/site-packages/rouge/rouge.py:120\u001b[0m, in \u001b[0;36mRouge._get_scores\u001b[0;34m(self, hyps, refs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[1;32m    119\u001b[0m     fn \u001b[38;5;241m=\u001b[39m Rouge\u001b[38;5;241m.\u001b[39mAVAILABLE_METRICS[m]\n\u001b[0;32m--> 120\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclusive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     sen_score[m] \u001b[38;5;241m=\u001b[39m {s: sc[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats}\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_lengths:\n",
      "File \u001b[0;32m~/code/cadastral-take-home/venv/lib/python3.11/site-packages/rouge/rouge.py:59\u001b[0m, in \u001b[0;36mRouge.<lambda>\u001b[0;34m(hyp, ref, **k)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRouge\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     DEFAULT_METRICS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m     AVAILABLE_METRICS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-3\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-4\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-5\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk:\n\u001b[0;32m---> 59\u001b[0m             \u001b[43mrouge_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrouge_l_summary_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     60\u001b[0m     }\n\u001b[1;32m     61\u001b[0m     DEFAULT_STATS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     62\u001b[0m     AVAILABLE_STATS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/code/cadastral-take-home/venv/lib/python3.11/site-packages/rouge/rouge_score.py:389\u001b[0m, in \u001b[0;36mrouge_l_summary_level\u001b[0;34m(evaluated_sentences, reference_sentences, raw_results, exclusive, **_)\u001b[0m\n\u001b[1;32m    387\u001b[0m union \u001b[38;5;241m=\u001b[39m Ngrams(exclusive\u001b[38;5;241m=\u001b[39mexclusive)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref_s \u001b[38;5;129;01min\u001b[39;00m reference_sentences:\n\u001b[0;32m--> 389\u001b[0m     lcs_count, union \u001b[38;5;241m=\u001b[39m \u001b[43m_union_lcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluated_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mref_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mprev_union\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mexclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclusive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     union_lcs_sum_across_all_references \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lcs_count\n\u001b[1;32m    395\u001b[0m llcs \u001b[38;5;241m=\u001b[39m union_lcs_sum_across_all_references\n",
      "File \u001b[0;32m~/code/cadastral-take-home/venv/lib/python3.11/site-packages/rouge/rouge_score.py:333\u001b[0m, in \u001b[0;36m_union_lcs\u001b[0;34m(evaluated_sentences, reference_sentence, prev_union, exclusive)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eval_s \u001b[38;5;129;01min\u001b[39;00m evaluated_sentences:\n\u001b[1;32m    332\u001b[0m     evaluated_words \u001b[38;5;241m=\u001b[39m _split_into_words([eval_s])\n\u001b[0;32m--> 333\u001b[0m     lcs \u001b[38;5;241m=\u001b[39m \u001b[43m_recon_lcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluated_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclusive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     combined_lcs_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lcs)\n\u001b[1;32m    335\u001b[0m     lcs_union \u001b[38;5;241m=\u001b[39m lcs_union\u001b[38;5;241m.\u001b[39munion(lcs)\n",
      "File \u001b[0;32m~/code/cadastral-take-home/venv/lib/python3.11/site-packages/rouge/rouge_score.py:177\u001b[0m, in \u001b[0;36m_recon_lcs\u001b[0;34m(x, y, exclusive)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mReturns the Longest Subsequence between x and y.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mSource: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m  sequence: LCS of x and y\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m i, j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[0;32m--> 177\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m_lcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recon\u001b[39m(i, j):\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"private recon calculation\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/code/cadastral-take-home/venv/lib/python3.11/site-packages/rouge/rouge_score.py:160\u001b[0m, in \u001b[0;36m_lcs\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    158\u001b[0m             table[i, j] \u001b[38;5;241m=\u001b[39m table[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m             table[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text by lowercasing and removing extra whitespace.\"\"\"\n",
    "    normalized = ' '.join(text.lower().split())\n",
    "    logging.debug(f\"Normalized text length: {len(normalized)}\")\n",
    "    return normalized\n",
    "\n",
    "def character_error_rate(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"Calculate the Character Error Rate (CER).\"\"\"\n",
    "    cer = Levenshtein.distance(reference, hypothesis) / len(reference)\n",
    "    logging.debug(f\"Calculated CER: {cer}\")\n",
    "    return cer\n",
    "\n",
    "def word_error_rate(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"Calculate the Word Error Rate (WER).\"\"\"\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    wer = Levenshtein.distance(ref_words, hyp_words) / len(ref_words)\n",
    "    logging.debug(f\"Calculated WER: {wer}\")\n",
    "    return wer\n",
    "\n",
    "def fuzzy_string_match(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"Calculate a fuzzy string match score using fuzzywuzzy.\"\"\"\n",
    "    score = fuzz.ratio(reference, hypothesis) / 100.0\n",
    "    logging.debug(f\"Calculated Fuzzy Match Score: {score}\")\n",
    "    return score\n",
    "\n",
    "def bleu_score(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"Calculate the BLEU score.\"\"\"\n",
    "    reference_words = reference.split()\n",
    "    hypothesis_words = hypothesis.split()\n",
    "    score = sentence_bleu([reference_words], hypothesis_words)\n",
    "    logging.debug(f\"Calculated BLEU Score: {score}\")\n",
    "    return score\n",
    "\n",
    "def rouge_scores(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "    \"\"\"Calculate ROUGE scores.\"\"\"\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)[0]\n",
    "    logging.debug(f\"Calculated ROUGE Scores: {scores}\")\n",
    "    return {\n",
    "        'rouge-1': scores['rouge-1']['f'],\n",
    "        'rouge-2': scores['rouge-2']['f'],\n",
    "        'rouge-l': scores['rouge-l']['f']\n",
    "    }\n",
    "\n",
    "def confidence_weighted_accuracy(reference: str, hypothesis: str, confidences: List[float]) -> Optional[float]:\n",
    "    \"\"\"Calculate the Confidence Weighted Accuracy (CWA).\"\"\"\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    \n",
    "    if len(hyp_words) != len(confidences):\n",
    "        logging.warning(f\"Number of words ({len(hyp_words)}) doesn't match number of confidence scores ({len(confidences)})\")\n",
    "        return None\n",
    "    \n",
    "    correct_confidence_sum = sum(conf for ref, hyp, conf in zip(ref_words, hyp_words, confidences) if ref == hyp)\n",
    "    total_confidence_sum = sum(confidences)\n",
    "    \n",
    "    if total_confidence_sum > 0:\n",
    "        cwa = correct_confidence_sum / total_confidence_sum\n",
    "    else:\n",
    "        cwa = 0.0\n",
    "    \n",
    "    logging.debug(f\"Calculated CWA: {cwa}\")\n",
    "    return cwa\n",
    "\n",
    "\n",
    "def evaluate_ocr(ground_truth: AnthropicResult, ocr_result: OCRResult) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate OCR results against ground truth.\"\"\"\n",
    "    logging.info(f\"Starting evaluation for {ocr_result.ocr_engine}\")\n",
    "    \n",
    "    gt_text = normalize_text(ground_truth.full_text)\n",
    "    ocr_text = normalize_text(ocr_result.ocr_text)\n",
    "    \n",
    "    logging.info(f\"Ground truth text length: {len(gt_text)}\")\n",
    "    logging.info(f\"OCR text length: {len(ocr_text)}\")\n",
    "    \n",
    "    evaluation = {}\n",
    "    \n",
    "    # Use tqdm to show progress for each metric calculation\n",
    "    metrics = ['CER', 'WER', 'Fuzzy_Match', 'BLEU', 'ROUGE']\n",
    "    for metric in tqdm(metrics, desc=f\"Calculating metrics for {ocr_result.ocr_engine}\"):\n",
    "        if metric == 'CER':\n",
    "            evaluation['CER'] = character_error_rate(gt_text, ocr_text)\n",
    "        elif metric == 'WER':\n",
    "            evaluation['WER'] = word_error_rate(gt_text, ocr_text)\n",
    "        elif metric == 'Fuzzy_Match':\n",
    "            evaluation['Fuzzy_Match'] = fuzzy_string_match(gt_text, ocr_text)\n",
    "        elif metric == 'BLEU':\n",
    "            evaluation['BLEU'] = bleu_score(gt_text, ocr_text)\n",
    "        elif metric == 'ROUGE':\n",
    "            evaluation.update(rouge_scores(gt_text, ocr_text))\n",
    "    \n",
    "    evaluation['Total_Time'] = ocr_result.total_time\n",
    "    evaluation['Avg_Time_Per_Page'] = np.mean([page.processing_time for page in ocr_result.page_results])\n",
    "    \n",
    "    \n",
    "    # Calculate CWA if confidence scores are available\n",
    "    confidences = []\n",
    "    for page in tqdm(ocr_result.page_results, desc=\"Extracting confidence scores\"):\n",
    "        if page.bounding_boxes:\n",
    "            confidences.extend([bb.confidence for bb in page.bounding_boxes if bb.confidence is not None])\n",
    "    \n",
    "    if confidences:\n",
    "        logging.info(f\"Number of confidence scores: {len(confidences)}\")\n",
    "        cwa = confidence_weighted_accuracy(gt_text, ocr_text, confidences)\n",
    "        if cwa is not None:\n",
    "            evaluation['CWA'] = cwa\n",
    "    else:\n",
    "        logging.warning(\"No confidence scores available for CWA calculation\")\n",
    "    \n",
    "    logging.info(f\"Evaluation results for {ocr_result.ocr_engine}: {evaluation}\")\n",
    "    return evaluation\n",
    "\n",
    "def evaluate_all_ocrs(ground_truth: AnthropicResult, ocr_results: Dict[str, OCRResult]) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate all OCR results and return a DataFrame with the results.\"\"\"\n",
    "    logging.info(\"Starting evaluation for all OCR engines\")\n",
    "    evaluations = {}\n",
    "    for ocr_name, ocr_result in tqdm(ocr_results.items(), desc=\"Evaluating OCR engines\"):\n",
    "        logging.info(f\"Evaluating {ocr_name}\")\n",
    "        evaluations[ocr_name] = evaluate_ocr(ground_truth, ocr_result)\n",
    "    \n",
    "    df = pd.DataFrame(evaluations).transpose()\n",
    "    logging.info(\"Completed evaluation for all OCR engines\")\n",
    "    logging.info(f\"Evaluation results:\\n{df}\")\n",
    "    return df\n",
    "\n",
    "def per_page_analysis(ground_truth: AnthropicResult, ocr_result: OCRResult) -> pd.DataFrame:\n",
    "    \"\"\"Perform detailed per-page analysis of OCR results.\"\"\"\n",
    "    logging.info(f\"Starting per-page analysis for {ocr_result.ocr_engine}\")\n",
    "    per_page_metrics = []\n",
    "    \n",
    "    # Ensure ground truth and OCR result have the same number of pages\n",
    "    if len(ground_truth.page_results) != len(ocr_result.page_results):\n",
    "        logging.warning(f\"Mismatch in number of pages: Ground Truth ({len(ground_truth.page_results)}) vs OCR ({len(ocr_result.page_results)})\")\n",
    "    \n",
    "    # Use the minimum number of pages to avoid index errors\n",
    "    num_pages = min(len(ground_truth.page_results), len(ocr_result.page_results))\n",
    "    \n",
    "    for i in tqdm(range(num_pages), desc=f\"Analyzing pages for {ocr_result.ocr_engine}\"):\n",
    "        gt_page = ground_truth.page_results[i]\n",
    "        ocr_page = ocr_result.page_results[i]\n",
    "        \n",
    "        gt_text = normalize_text(gt_page.text)\n",
    "        ocr_text = normalize_text(ocr_page.text)\n",
    "        \n",
    "        logging.debug(f\"Page {gt_page.page_number}: GT length: {len(gt_text)}, OCR length: {len(ocr_text)}\")\n",
    "        \n",
    "        metrics = {\n",
    "            'Page': gt_page.page_number,\n",
    "            'CER': character_error_rate(gt_text, ocr_text),\n",
    "            'WER': word_error_rate(gt_text, ocr_text),\n",
    "            'Fuzzy_Match': fuzzy_string_match(gt_text, ocr_text),\n",
    "            'BLEU': bleu_score(gt_text, ocr_text),\n",
    "            'Processing_Time': ocr_page.processing_time\n",
    "        }\n",
    "        \n",
    "        # Add ROUGE scores\n",
    "        rouge_scores_dict = rouge_scores(gt_text, ocr_text)\n",
    "        metrics.update({f\"ROUGE-{k}\": v for k, v in rouge_scores_dict.items()})\n",
    "        \n",
    "        if hasattr(ocr_page, 'avg_confidence'):\n",
    "            metrics['Avg_Confidence'] = ocr_page.avg_confidence\n",
    "        \n",
    "        # Calculate CWA if confidence scores are available for this page\n",
    "        if ocr_page.bounding_boxes:\n",
    "            confidences = [bb.confidence for bb in ocr_page.bounding_boxes if bb.confidence is not None]\n",
    "            if confidences:\n",
    "                cwa = confidence_weighted_accuracy(gt_text, ocr_text, confidences)\n",
    "                if cwa is not None:\n",
    "                    metrics['CWA'] = cwa\n",
    "        \n",
    "        per_page_metrics.append(metrics)\n",
    "        \n",
    "        logging.debug(f\"Page {gt_page.page_number} metrics: {metrics}\")\n",
    "    \n",
    "    df = pd.DataFrame(per_page_metrics)\n",
    "    logging.info(f\"Completed per-page analysis for {ocr_result.ocr_engine}\")\n",
    "    return df\n",
    "\n",
    "# Main execution flow\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Path to your PDF file\n",
    "        pdf_path = \"bagel_jays.pdf\"\n",
    "\n",
    "        logging.info(\"Extracting text from the PDF...\")\n",
    "\n",
    "        # Ground Truth\n",
    "        ground_truth_checkpoint = 'ground_truth_checkpoint.pkl'\n",
    "        ground_truth = load_checkpoint(ground_truth_checkpoint)\n",
    "        if ground_truth is None:\n",
    "            ground_truth = get_anthropic_ground_truth(pdf_path)\n",
    "            save_checkpoint(ground_truth, ground_truth_checkpoint)\n",
    "\n",
    "        # Tesseract OCR\n",
    "        tesseract_checkpoint = 'tesseract_checkpoint.pkl'\n",
    "        tesseract_result = load_checkpoint(tesseract_checkpoint)\n",
    "        if tesseract_result is None:\n",
    "            tesseract_ocr = TesseractOCR()\n",
    "            tesseract_result = tesseract_ocr.perform_ocr(pdf_path)\n",
    "            save_checkpoint(tesseract_result, tesseract_checkpoint)\n",
    "\n",
    "        # MathPix OCR\n",
    "        mathpix_checkpoint = 'mathpix_checkpoint.pkl'\n",
    "        mathpix_result = load_checkpoint(mathpix_checkpoint)\n",
    "        if mathpix_result is None:\n",
    "            mathpix_ocr = MathpixOCR(MATHPIX_APP_ID, MATHPIX_APP_KEY)\n",
    "            mathpix_result = mathpix_ocr.perform_ocr(pdf_path)\n",
    "            save_checkpoint(mathpix_result, mathpix_checkpoint)\n",
    "\n",
    "        # Google Cloud Vision OCR\n",
    "        gcv_checkpoint = 'gcv_checkpoint.pkl'\n",
    "        gcv_result = load_checkpoint(gcv_checkpoint)\n",
    "        if gcv_result is None:\n",
    "            gcv_ocr = GoogleCloudVisionOCR()\n",
    "            gcv_result = gcv_ocr.perform_ocr(pdf_path)\n",
    "            save_checkpoint(gcv_result, gcv_checkpoint)\n",
    "        \n",
    "        ocr_results = {\n",
    "            'Tesseract': tesseract_result,\n",
    "            'Mathpix': mathpix_result,\n",
    "            'Google Cloud Vision': gcv_result\n",
    "        }\n",
    "\n",
    "        logging.info(f\"Text fully extracted. OCR Evaluation starting...\")\n",
    "\n",
    "        # Evaluation\n",
    "        evaluation_checkpoint = 'evaluation_checkpoint.pkl'\n",
    "        evaluation_df = load_checkpoint(evaluation_checkpoint)\n",
    "        if evaluation_df is None:\n",
    "            evaluation_df = evaluate_all_ocrs(ground_truth, ocr_results)\n",
    "            save_checkpoint(evaluation_df, evaluation_checkpoint)\n",
    "        \n",
    "        logging.info(f\"OCR evaluation process completed\")\n",
    "        print(\"OCR Evaluation Results:\")\n",
    "        print(evaluation_df)\n",
    "        \n",
    "        # Visualization of overall results\n",
    "        metrics = ['CER', 'WER', 'Fuzzy_Match', 'Total_Time', 'Avg_Time_Per_Page']\n",
    "        \n",
    "        fig, axes = plt.subplots(len(metrics), 1, figsize=(10, 5*len(metrics)))\n",
    "        for i, metric in enumerate(metrics):\n",
    "            evaluation_df[metric].plot(kind='bar', ax=axes[i], title=f'{metric} Comparison')\n",
    "            axes[i].set_ylabel(metric)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        logging.info(\"Starting per-page analysis\")\n",
    "\n",
    "        # Perform per-page analysis for each OCR method\n",
    "        logging.info(\"Starting per-page analysis for all OCR engines\")\n",
    "        per_page_checkpoint = 'per_page_checkpoint.pkl'\n",
    "        per_page_results = load_checkpoint(per_page_checkpoint)\n",
    "        if per_page_results is None:\n",
    "            per_page_results = {}\n",
    "            for ocr_name, ocr_result in tqdm(ocr_results.items(), desc=\"Analyzing OCR engines\"):\n",
    "                per_page_results[ocr_name] = per_page_analysis(ground_truth, ocr_result)\n",
    "\n",
    "                print(f\"\\nPer-page analysis summary for {ocr_name}:\")\n",
    "                print(per_page_results[ocr_name].describe())\n",
    "            save_checkpoint(per_page_results, per_page_checkpoint)\n",
    "\n",
    "        logging.info(\"Per-page analysis completed\")\n",
    "\n",
    "        for ocr_name, per_page_df in per_page_results.items():\n",
    "            print(f\"\\nPer-page analysis for {ocr_name}:\")\n",
    "            print(per_page_df)\n",
    "            \n",
    "            # Visualize per-page results\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            per_page_df.plot(x='Page', y='CER', ax=axes[0, 0], title='CER by Page')\n",
    "            per_page_df.plot(x='Page', y='WER', ax=axes[0, 1], title='WER by Page')\n",
    "            per_page_df.plot(x='Page', y='Fuzzy_Match', ax=axes[1, 0], title='Fuzzy Match by Page')\n",
    "            per_page_df.plot(x='Page', y='Processing_Time', ax=axes[1, 1], title='Processing Time by Page')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during execution: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a10ca0-bf43-48d8-bce9-eb654e9d177c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
